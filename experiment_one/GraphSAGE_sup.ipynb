{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Users\\\\csaba\\\\Documents\\\\Coding\\\\git_own\\\\thesis_coding\\\\utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import importlib\n",
    "\n",
    "import training\n",
    "importlib.reload(training)\n",
    "import graph_vis\n",
    "importlib.reload(graph_vis)\n",
    "import graph_creation\n",
    "importlib.reload(graph_creation)\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "device = utils.set_seeds_and_device() \n",
    "dataset,data = training.load_dataset('Cora', \"./training_data/datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    # cross entroy loss makes this model supervised \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Test function\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = pred[mask] == data.y[mask]\n",
    "        accs.append(int(correct.sum()) / int(mask.sum()))\n",
    "    return accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, Loss: 1.9492, Train: 0.6643, Val: 0.4780, Test: 0.4940\n",
      "Epoch 002, Loss: 1.8872, Train: 0.6786, Val: 0.4860, Test: 0.4830\n",
      "Epoch 003, Loss: 1.6736, Train: 0.7571, Val: 0.5900, Test: 0.6070\n",
      "Epoch 004, Loss: 1.2094, Train: 0.8714, Val: 0.6200, Test: 0.6490\n",
      "Epoch 005, Loss: 0.6999, Train: 0.9571, Val: 0.6120, Test: 0.6330\n",
      "Epoch 006, Loss: 0.3930, Train: 0.9357, Val: 0.7120, Test: 0.7470\n",
      "Epoch 007, Loss: 0.2646, Train: 0.9857, Val: 0.7100, Test: 0.7240\n",
      "Epoch 008, Loss: 0.0930, Train: 1.0000, Val: 0.7200, Test: 0.7130\n",
      "Epoch 009, Loss: 0.0293, Train: 0.9929, Val: 0.7220, Test: 0.7180\n",
      "Epoch 010, Loss: 0.0179, Train: 1.0000, Val: 0.6920, Test: 0.7110\n",
      "Epoch 011, Loss: 0.0108, Train: 1.0000, Val: 0.6940, Test: 0.7110\n",
      "Epoch 012, Loss: 0.0076, Train: 1.0000, Val: 0.6980, Test: 0.7180\n",
      "Epoch 013, Loss: 0.0024, Train: 1.0000, Val: 0.6900, Test: 0.7150\n",
      "Epoch 014, Loss: 0.0006, Train: 1.0000, Val: 0.6940, Test: 0.7230\n",
      "Epoch 015, Loss: 0.0003, Train: 1.0000, Val: 0.7040, Test: 0.7270\n",
      "Epoch 016, Loss: 0.0003, Train: 1.0000, Val: 0.7100, Test: 0.7350\n",
      "Epoch 017, Loss: 0.0005, Train: 1.0000, Val: 0.7100, Test: 0.7420\n",
      "Epoch 018, Loss: 0.0006, Train: 1.0000, Val: 0.7160, Test: 0.7430\n",
      "Epoch 019, Loss: 0.0003, Train: 1.0000, Val: 0.7180, Test: 0.7490\n",
      "Epoch 020, Loss: 0.0002, Train: 1.0000, Val: 0.7240, Test: 0.7550\n",
      "Epoch 021, Loss: 0.0002, Train: 1.0000, Val: 0.7420, Test: 0.7600\n",
      "Epoch 022, Loss: 0.0001, Train: 1.0000, Val: 0.7480, Test: 0.7690\n",
      "Epoch 023, Loss: 0.0001, Train: 1.0000, Val: 0.7480, Test: 0.7790\n",
      "Epoch 024, Loss: 0.0000, Train: 1.0000, Val: 0.7620, Test: 0.7890\n",
      "Epoch 025, Loss: 0.0000, Train: 1.0000, Val: 0.7600, Test: 0.7890\n",
      "Epoch 026, Loss: 0.0000, Train: 1.0000, Val: 0.7600, Test: 0.7940\n",
      "Epoch 027, Loss: 0.0000, Train: 1.0000, Val: 0.7620, Test: 0.7940\n",
      "Epoch 028, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7960\n",
      "Epoch 029, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7930\n",
      "Epoch 030, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 031, Loss: 0.0000, Train: 1.0000, Val: 0.7700, Test: 0.7960\n",
      "Epoch 032, Loss: 0.0000, Train: 1.0000, Val: 0.7700, Test: 0.8000\n",
      "Epoch 033, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.8000\n",
      "Epoch 034, Loss: 0.0000, Train: 1.0000, Val: 0.7740, Test: 0.7980\n",
      "Epoch 035, Loss: 0.0000, Train: 1.0000, Val: 0.7740, Test: 0.7990\n",
      "Epoch 036, Loss: 0.0000, Train: 1.0000, Val: 0.7760, Test: 0.7980\n",
      "Epoch 037, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7990\n",
      "Epoch 038, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7970\n",
      "Epoch 039, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7980\n",
      "Epoch 040, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7990\n",
      "Epoch 041, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.8000\n",
      "Epoch 042, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.8000\n",
      "Epoch 043, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7990\n",
      "Epoch 044, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7990\n",
      "Epoch 045, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7970\n",
      "Epoch 046, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7970\n",
      "Epoch 047, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7970\n",
      "Epoch 048, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7970\n",
      "Epoch 049, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7970\n",
      "Epoch 050, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7960\n",
      "Epoch 051, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7960\n",
      "Epoch 052, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7960\n",
      "Epoch 053, Loss: 0.0000, Train: 1.0000, Val: 0.7740, Test: 0.7960\n",
      "Epoch 054, Loss: 0.0000, Train: 1.0000, Val: 0.7720, Test: 0.7960\n",
      "Epoch 055, Loss: 0.0000, Train: 1.0000, Val: 0.7700, Test: 0.7960\n",
      "Epoch 056, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7960\n",
      "Epoch 057, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 058, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 059, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 060, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 061, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 062, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 063, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 064, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 065, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 066, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 067, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 068, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 069, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 070, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 071, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7940\n",
      "Epoch 072, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 073, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 074, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 075, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 076, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 077, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 078, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 079, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 080, Loss: 0.0000, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
      "Epoch 081, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 082, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 083, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 084, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 085, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 086, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 087, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 088, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 089, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 090, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 091, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 092, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 093, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 094, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 095, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 096, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 097, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 098, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 099, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n",
      "Epoch 100, Loss: 0.0000, Train: 1.0000, Val: 0.7660, Test: 0.7950\n"
     ]
    }
   ],
   "source": [
    "# Define the GraphSAGE model\n",
    "model = GraphSAGE(\n",
    "    in_channels=dataset.num_features,\n",
    "    hidden_channels=64,\n",
    "    num_layers=4,\n",
    "    out_channels=dataset.num_classes,\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, Loss: 1.9536, Train: 0.9929, Val: 0.6520, Test: 0.6460\n",
      "Epoch 002, Loss: 1.5785, Train: 0.9929, Val: 0.7120, Test: 0.7120\n",
      "Epoch 003, Loss: 1.0439, Train: 0.9929, Val: 0.7320, Test: 0.7250\n",
      "Epoch 004, Loss: 0.5554, Train: 0.9929, Val: 0.7500, Test: 0.7510\n",
      "Epoch 005, Loss: 0.2430, Train: 1.0000, Val: 0.7680, Test: 0.7670\n",
      "Epoch 006, Loss: 0.0936, Train: 1.0000, Val: 0.7640, Test: 0.7720\n",
      "Epoch 007, Loss: 0.0351, Train: 1.0000, Val: 0.7640, Test: 0.7710\n",
      "Epoch 008, Loss: 0.0135, Train: 1.0000, Val: 0.7680, Test: 0.7720\n",
      "Epoch 009, Loss: 0.0055, Train: 1.0000, Val: 0.7700, Test: 0.7760\n",
      "Epoch 010, Loss: 0.0024, Train: 1.0000, Val: 0.7640, Test: 0.7760\n",
      "Epoch 011, Loss: 0.0011, Train: 1.0000, Val: 0.7580, Test: 0.7720\n",
      "Epoch 012, Loss: 0.0006, Train: 1.0000, Val: 0.7540, Test: 0.7700\n",
      "Epoch 013, Loss: 0.0003, Train: 1.0000, Val: 0.7500, Test: 0.7700\n",
      "Epoch 014, Loss: 0.0002, Train: 1.0000, Val: 0.7460, Test: 0.7680\n",
      "Epoch 015, Loss: 0.0001, Train: 1.0000, Val: 0.7460, Test: 0.7630\n",
      "Epoch 016, Loss: 0.0001, Train: 1.0000, Val: 0.7500, Test: 0.7620\n",
      "Epoch 017, Loss: 0.0000, Train: 1.0000, Val: 0.7520, Test: 0.7620\n",
      "Epoch 018, Loss: 0.0000, Train: 1.0000, Val: 0.7520, Test: 0.7620\n",
      "Epoch 019, Loss: 0.0000, Train: 1.0000, Val: 0.7520, Test: 0.7600\n",
      "Epoch 020, Loss: 0.0000, Train: 1.0000, Val: 0.7520, Test: 0.7590\n",
      "Epoch 021, Loss: 0.0000, Train: 1.0000, Val: 0.7520, Test: 0.7590\n",
      "Epoch 022, Loss: 0.0000, Train: 1.0000, Val: 0.7520, Test: 0.7580\n",
      "Epoch 023, Loss: 0.0000, Train: 1.0000, Val: 0.7520, Test: 0.7580\n",
      "Epoch 024, Loss: 0.0000, Train: 1.0000, Val: 0.7520, Test: 0.7590\n",
      "Epoch 025, Loss: 0.0000, Train: 1.0000, Val: 0.7500, Test: 0.7590\n",
      "Epoch 026, Loss: 0.0000, Train: 1.0000, Val: 0.7480, Test: 0.7590\n",
      "Epoch 027, Loss: 0.0000, Train: 1.0000, Val: 0.7480, Test: 0.7590\n",
      "Epoch 028, Loss: 0.0000, Train: 1.0000, Val: 0.7480, Test: 0.7600\n",
      "Epoch 029, Loss: 0.0000, Train: 1.0000, Val: 0.7480, Test: 0.7600\n",
      "Epoch 030, Loss: 0.0000, Train: 1.0000, Val: 0.7480, Test: 0.7600\n",
      "Epoch 031, Loss: 0.0000, Train: 1.0000, Val: 0.7460, Test: 0.7600\n",
      "Epoch 032, Loss: 0.0000, Train: 1.0000, Val: 0.7440, Test: 0.7600\n",
      "Epoch 033, Loss: 0.0000, Train: 1.0000, Val: 0.7440, Test: 0.7590\n",
      "Epoch 034, Loss: 0.0000, Train: 1.0000, Val: 0.7440, Test: 0.7580\n",
      "Epoch 035, Loss: 0.0000, Train: 1.0000, Val: 0.7440, Test: 0.7570\n",
      "Epoch 036, Loss: 0.0000, Train: 1.0000, Val: 0.7440, Test: 0.7570\n",
      "Epoch 037, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7570\n",
      "Epoch 038, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 039, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 040, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 041, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 042, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 043, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 044, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 045, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 046, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 047, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 048, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 049, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 050, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 051, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 052, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 053, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 054, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 055, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 056, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 057, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 058, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7560\n",
      "Epoch 059, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 060, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 061, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 062, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 063, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 064, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 065, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 066, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 067, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 068, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 069, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 070, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 071, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 072, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 073, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 074, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 075, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 076, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 077, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 078, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 079, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 080, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 081, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 082, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 083, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 084, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 085, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 086, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 087, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 088, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 089, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 090, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 091, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 092, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 093, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 094, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 095, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 096, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 097, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 098, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 099, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 100, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 101, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 102, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 103, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 104, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 105, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 106, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 107, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 108, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 109, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 110, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 111, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 112, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 113, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 114, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 115, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 116, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 117, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 118, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 119, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 120, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 121, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 122, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 123, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 124, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 125, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 126, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 127, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 128, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 129, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 130, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 131, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 132, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 133, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 134, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 135, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 136, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 137, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 138, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 139, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 140, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 141, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 142, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 143, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 144, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 145, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 146, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 147, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 148, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 149, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 150, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 151, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 152, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 153, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 154, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 155, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 156, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 157, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 158, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 159, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 160, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 161, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 162, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 163, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 164, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 165, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 166, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 167, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 168, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 169, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 170, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 171, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7540\n",
      "Epoch 172, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 173, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 174, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 175, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 176, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 177, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 178, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 179, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 180, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 181, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 182, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 183, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 184, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 185, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 186, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 187, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 188, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 189, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 190, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 191, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 192, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 193, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 194, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 195, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 196, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 197, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 198, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 199, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Epoch 200, Loss: 0.0000, Train: 1.0000, Val: 0.7420, Test: 0.7550\n",
      "Final node embeddings shape: (2708, 7)\n",
      "Final node embeddings: [[ -9.487305   -1.7762113  -7.891106  ...  -3.4458225  -8.613261\n",
      "   -2.5700634]\n",
      " [ -7.7403355  -7.4230356  -9.984571  ...  25.200691  -13.318379\n",
      "   -8.28144  ]\n",
      " [ -7.488588   -7.6688857  -9.208262  ...  18.581917  -10.7005825\n",
      "   -7.1821547]\n",
      " ...\n",
      " [ -1.4790068   0.4136443 -10.13559   ...  -3.063224   -0.2538945\n",
      "   -2.9397335]\n",
      " [ -6.4558816  -2.276296   -5.202525  ...   2.4233665  -4.59303\n",
      "   -5.459449 ]\n",
      " [ -8.100384   -0.5956931  -7.0227814 ...   1.4240158  -5.855771\n",
      "   -5.594131 ]]\n",
      "Training complete!\n",
      "Epoch 001, Loss: 1.9296, Train: 0.8286, Val: 0.3880, Test: 0.4030\n",
      "Epoch 002, Loss: 1.1912, Train: 0.9714, Val: 0.6520, Test: 0.6590\n",
      "Epoch 003, Loss: 0.5953, Train: 0.9643, Val: 0.6540, Test: 0.6840\n",
      "Epoch 004, Loss: 0.3063, Train: 0.9929, Val: 0.6820, Test: 0.6780\n",
      "Epoch 005, Loss: 0.1786, Train: 0.9929, Val: 0.7020, Test: 0.6870\n",
      "Epoch 006, Loss: 0.1350, Train: 0.9929, Val: 0.7380, Test: 0.7100\n",
      "Epoch 007, Loss: 0.0705, Train: 1.0000, Val: 0.7460, Test: 0.7490\n",
      "Epoch 008, Loss: 0.0132, Train: 1.0000, Val: 0.7700, Test: 0.7610\n",
      "Epoch 009, Loss: 0.0085, Train: 1.0000, Val: 0.7640, Test: 0.7590\n",
      "Epoch 010, Loss: 0.0040, Train: 1.0000, Val: 0.7400, Test: 0.7600\n",
      "Epoch 011, Loss: 0.0028, Train: 1.0000, Val: 0.7280, Test: 0.7440\n",
      "Epoch 012, Loss: 0.0028, Train: 1.0000, Val: 0.7200, Test: 0.7320\n",
      "Epoch 013, Loss: 0.0022, Train: 1.0000, Val: 0.7180, Test: 0.7310\n",
      "Epoch 014, Loss: 0.0013, Train: 1.0000, Val: 0.7140, Test: 0.7310\n",
      "Epoch 015, Loss: 0.0008, Train: 1.0000, Val: 0.7080, Test: 0.7290\n",
      "Epoch 016, Loss: 0.0005, Train: 1.0000, Val: 0.7100, Test: 0.7260\n",
      "Epoch 017, Loss: 0.0003, Train: 1.0000, Val: 0.7120, Test: 0.7200\n",
      "Epoch 018, Loss: 0.0002, Train: 1.0000, Val: 0.7140, Test: 0.7230\n",
      "Epoch 019, Loss: 0.0001, Train: 1.0000, Val: 0.7120, Test: 0.7240\n",
      "Epoch 020, Loss: 0.0001, Train: 1.0000, Val: 0.7120, Test: 0.7200\n",
      "Epoch 021, Loss: 0.0000, Train: 1.0000, Val: 0.7120, Test: 0.7190\n",
      "Epoch 022, Loss: 0.0000, Train: 1.0000, Val: 0.7140, Test: 0.7220\n",
      "Epoch 023, Loss: 0.0000, Train: 1.0000, Val: 0.7140, Test: 0.7240\n",
      "Epoch 024, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7270\n",
      "Epoch 025, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7300\n",
      "Epoch 026, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7300\n",
      "Epoch 027, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7310\n",
      "Epoch 028, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7330\n",
      "Epoch 029, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7330\n",
      "Epoch 030, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7310\n",
      "Epoch 031, Loss: 0.0000, Train: 1.0000, Val: 0.7280, Test: 0.7270\n",
      "Epoch 032, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7280\n",
      "Epoch 033, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7290\n",
      "Epoch 034, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7300\n",
      "Epoch 035, Loss: 0.0000, Train: 1.0000, Val: 0.7280, Test: 0.7290\n",
      "Epoch 036, Loss: 0.0000, Train: 1.0000, Val: 0.7300, Test: 0.7300\n",
      "Epoch 037, Loss: 0.0000, Train: 1.0000, Val: 0.7300, Test: 0.7310\n",
      "Epoch 038, Loss: 0.0000, Train: 1.0000, Val: 0.7300, Test: 0.7310\n",
      "Epoch 039, Loss: 0.0000, Train: 1.0000, Val: 0.7300, Test: 0.7300\n",
      "Epoch 040, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7320\n",
      "Epoch 041, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7310\n",
      "Epoch 042, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7320\n",
      "Epoch 043, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7310\n",
      "Epoch 044, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7300\n",
      "Epoch 045, Loss: 0.0000, Train: 1.0000, Val: 0.7260, Test: 0.7290\n",
      "Epoch 046, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 047, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 048, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 049, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 050, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 051, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 052, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 053, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7290\n",
      "Epoch 054, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 055, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 056, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 057, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 058, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 059, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7290\n",
      "Epoch 060, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7290\n",
      "Epoch 061, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7290\n",
      "Epoch 062, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7290\n",
      "Epoch 063, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7290\n",
      "Epoch 064, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7290\n",
      "Epoch 065, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7290\n",
      "Epoch 066, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7290\n",
      "Epoch 067, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 068, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 069, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 070, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 071, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 072, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 073, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 074, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 075, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 076, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 077, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 078, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 079, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 080, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 081, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 082, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 083, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 084, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 085, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 086, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 087, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 088, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 089, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 090, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 091, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 092, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 093, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 094, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 095, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 096, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 097, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 098, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 099, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 100, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 101, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 102, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 103, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 104, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 105, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 106, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 107, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 108, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 109, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 110, Loss: 0.0000, Train: 1.0000, Val: 0.7200, Test: 0.7300\n",
      "Epoch 111, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 112, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 113, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 114, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 115, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 116, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 117, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 118, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 119, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 120, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 121, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 122, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 123, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 124, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 125, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 126, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 127, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 128, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 129, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 130, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 131, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 132, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 133, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 134, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 135, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 136, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 137, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 138, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 139, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7310\n",
      "Epoch 140, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7310\n",
      "Epoch 141, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7310\n",
      "Epoch 142, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 143, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7320\n",
      "Epoch 144, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7320\n",
      "Epoch 145, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7320\n",
      "Epoch 146, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7320\n",
      "Epoch 147, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7320\n",
      "Epoch 148, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7320\n",
      "Epoch 149, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7320\n",
      "Epoch 150, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 151, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 152, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 153, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 154, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 155, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 156, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 157, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 158, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 159, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 160, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 161, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 162, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 163, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 164, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 165, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 166, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 167, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 168, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 169, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 170, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 171, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 172, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 173, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 174, Loss: 0.0000, Train: 1.0000, Val: 0.7220, Test: 0.7300\n",
      "Epoch 175, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 176, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 177, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 178, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 179, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 180, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 181, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 182, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 183, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 184, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 185, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 186, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 187, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 188, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 189, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 190, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 191, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7300\n",
      "Epoch 192, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 193, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 194, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 195, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 196, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 197, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 198, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 199, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Epoch 200, Loss: 0.0000, Train: 1.0000, Val: 0.7240, Test: 0.7310\n",
      "Final node embeddings shape: (2708, 7)\n",
      "Final node embeddings: [[ -2.633264    -2.8225307   -9.482664   ...  -3.3983994  -10.717613\n",
      "   -6.4355226 ]\n",
      " [  0.85662955 -10.0513735  -11.322006   ...  25.349537   -14.198039\n",
      "  -13.119038  ]\n",
      " [ -1.2394309  -26.354763    -6.2677803  ...  30.007236   -17.507254\n",
      "  -18.945358  ]\n",
      " ...\n",
      " [  1.5347266   -1.452297    -3.8501098  ...  -2.564642    -0.12165356\n",
      "   -2.4312236 ]\n",
      " [ -1.1791145   -6.5059285   -6.471888   ...   4.775177    -8.160864\n",
      "   -7.8603396 ]\n",
      " [ -7.3341594  -12.730569   -12.018438   ...  -0.2735752   -9.50376\n",
      "   -5.881567  ]]\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 aggrs, at the end create illsutration and get embeddings\n",
    "for i in ['mean','max','lstm']:\n",
    "    # Define the GraphSAGE model\n",
    "    model = GraphSAGE(\n",
    "        in_channels=dataset.num_features,\n",
    "        hidden_channels=64,\n",
    "        num_layers=2,\n",
    "        aggr=i,\n",
    "        out_channels=dataset.num_classes,\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        train_acc, val_acc, test_acc = test()\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, '\n",
    "            f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        final_embeddings = model(data.x, data.edge_index).cpu().numpy() # Get embeddings for all nodes and move to CPU/NumPy\n",
    "    print(f\"Final node embeddings shape: {final_embeddings.shape}\") # Print the shape of the final embeddings\n",
    "    print(f\"Final node embeddings: {final_embeddings}\") # Print the final node embeddings\n",
    "\n",
    "    print(\"Training complete!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-thesis_coding-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
