{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18067f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'training' from 'c:\\\\Users\\\\csaba\\\\Documents\\\\Coding\\\\git_own\\\\thesis_coding\\\\training.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "\n",
    "import importlib\n",
    "\n",
    "import training\n",
    "importlib.reload(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9760d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,data = training.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e99ce8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 4.7273, Val: 0.4780, Test: 0.5090\n",
      "Epoch: 002, Loss: 4.1412, Val: 0.5000, Test: 0.5570\n",
      "Epoch: 003, Loss: 3.9487, Val: 0.5760, Test: 0.6000\n",
      "Epoch: 004, Loss: 3.8380, Val: 0.5360, Test: 0.5920\n",
      "Epoch: 005, Loss: 3.8187, Val: 0.5540, Test: 0.6000\n",
      "Epoch: 006, Loss: 3.7917, Val: 0.5600, Test: 0.5780\n",
      "Epoch: 007, Loss: 3.7354, Val: 0.5460, Test: 0.5810\n",
      "Epoch: 008, Loss: 3.7152, Val: 0.5640, Test: 0.5910\n",
      "Epoch: 009, Loss: 3.7440, Val: 0.5580, Test: 0.5770\n",
      "Epoch: 010, Loss: 3.7595, Val: 0.6100, Test: 0.6140\n",
      "Epoch: 011, Loss: 3.7417, Val: 0.5800, Test: 0.6150\n",
      "Epoch: 012, Loss: 3.7492, Val: 0.5760, Test: 0.5960\n",
      "Epoch: 013, Loss: 3.7363, Val: 0.5920, Test: 0.5930\n",
      "Epoch: 014, Loss: 3.7140, Val: 0.5760, Test: 0.5800\n",
      "Epoch: 015, Loss: 3.7112, Val: 0.5940, Test: 0.5780\n",
      "Epoch: 016, Loss: 3.6711, Val: 0.5880, Test: 0.5940\n",
      "Epoch: 017, Loss: 3.6874, Val: 0.5520, Test: 0.5710\n",
      "Epoch: 018, Loss: 3.7268, Val: 0.5720, Test: 0.5850\n",
      "Epoch: 019, Loss: 3.7358, Val: 0.5680, Test: 0.5650\n",
      "Epoch: 020, Loss: 3.7190, Val: 0.5740, Test: 0.5750\n",
      "Epoch: 021, Loss: 3.6692, Val: 0.5600, Test: 0.5840\n",
      "Epoch: 022, Loss: 3.6381, Val: 0.5580, Test: 0.5730\n",
      "Epoch: 023, Loss: 3.6540, Val: 0.5680, Test: 0.5590\n",
      "Epoch: 024, Loss: 3.6661, Val: 0.5480, Test: 0.5520\n",
      "Epoch: 025, Loss: 3.6264, Val: 0.5600, Test: 0.5520\n",
      "Epoch: 026, Loss: 3.6568, Val: 0.5600, Test: 0.5850\n",
      "Epoch: 027, Loss: 3.6095, Val: 0.5680, Test: 0.5910\n",
      "Epoch: 028, Loss: 3.6930, Val: 0.5480, Test: 0.5660\n",
      "Epoch: 029, Loss: 3.6414, Val: 0.5600, Test: 0.5600\n",
      "Epoch: 030, Loss: 3.6665, Val: 0.5400, Test: 0.5570\n",
      "Epoch: 031, Loss: 3.6267, Val: 0.5380, Test: 0.5390\n",
      "Epoch: 032, Loss: 3.6555, Val: 0.5480, Test: 0.5320\n",
      "Epoch: 033, Loss: 3.6778, Val: 0.5620, Test: 0.5470\n",
      "Epoch: 034, Loss: 3.7086, Val: 0.5700, Test: 0.5810\n",
      "Epoch: 035, Loss: 3.6690, Val: 0.5580, Test: 0.5400\n",
      "Epoch: 036, Loss: 3.6433, Val: 0.5580, Test: 0.5560\n",
      "Epoch: 037, Loss: 3.6402, Val: 0.5420, Test: 0.5660\n",
      "Epoch: 038, Loss: 3.6046, Val: 0.5460, Test: 0.5520\n",
      "Epoch: 039, Loss: 3.6220, Val: 0.5620, Test: 0.5670\n",
      "Epoch: 040, Loss: 3.6139, Val: 0.5500, Test: 0.5490\n",
      "Epoch: 041, Loss: 3.6594, Val: 0.5720, Test: 0.5570\n",
      "Epoch: 042, Loss: 3.6417, Val: 0.5320, Test: 0.5370\n",
      "Epoch: 043, Loss: 3.6074, Val: 0.5360, Test: 0.5520\n",
      "Epoch: 044, Loss: 3.5627, Val: 0.5160, Test: 0.5180\n",
      "Epoch: 045, Loss: 3.6309, Val: 0.5360, Test: 0.5310\n",
      "Epoch: 046, Loss: 3.6162, Val: 0.5520, Test: 0.5510\n",
      "Epoch: 047, Loss: 3.6500, Val: 0.5540, Test: 0.5520\n",
      "Epoch: 048, Loss: 3.6524, Val: 0.5420, Test: 0.5340\n",
      "Epoch: 049, Loss: 3.6303, Val: 0.5220, Test: 0.5380\n",
      "Epoch: 050, Loss: 3.5582, Val: 0.5300, Test: 0.5320\n",
      "Median time per epoch: 1.4689s\n"
     ]
    }
   ],
   "source": [
    "# Link prediction loader (self-supervised)\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    ")\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device, 'x', 'edge_index')\n",
    "\n",
    "# GraphSAGE encoder (no classification head)\n",
    "model = GraphSAGE(\n",
    "    in_channels=dataset.num_features,\n",
    "    hidden_channels=64,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Self-supervised training via link prediction\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        h = model(batch.x, batch.edge_index)\n",
    "        h_src = h[batch.edge_label_index[0]]\n",
    "        h_dst = h[batch.edge_label_index[1]]\n",
    "        pred = (h_src * h_dst).sum(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "    return total_loss / data.num_nodes\n",
    "\n",
    "# Evaluation: use learned embeddings for node classification\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    h = model(data.x, data.edge_index).cpu().numpy()\n",
    "    y = data.y.cpu().numpy()\n",
    "    clf = LogisticRegression(max_iter=5000)\n",
    "    clf.fit(h[data.train_mask.cpu().numpy()], y[data.train_mask.cpu().numpy()])\n",
    "    val_acc = clf.score(h[data.val_mask.cpu().numpy()], y[data.val_mask.cpu().numpy()])\n",
    "    test_acc = clf.score(h[data.test_mask.cpu().numpy()], y[data.test_mask.cpu().numpy()])\n",
    "    return float(val_acc), float(test_acc)\n",
    "\n",
    "# Training loop\n",
    "times = []\n",
    "for epoch in range(1, 51):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    val_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe13f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba723a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd277b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3f45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-thesis_coding-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
